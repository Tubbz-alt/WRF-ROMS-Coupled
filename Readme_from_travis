Configuration and Running Steps:
===============================
(1) To compile WRF, I used:
<><><><><> module load roms

<><><><><> ./build_wrf_copy.sh -move -j 4 | tee build_wrf.log

Please select from among the following Linux x86_64 options:

  1. (serial)   2. (smpar)   3. (dmpar)   4. (dm+sm)   PGI (pgf90/gcc)
  5. (serial)   6. (smpar)   7. (dmpar)   8. (dm+sm)   PGI (pgf90/pgcc): SGI MPT
  9. (serial)  10. (smpar)  11. (dmpar)  12. (dm+sm)   PGI (pgf90/gcc): PGI accelerator
 13. (serial)  14. (smpar)  15. (dmpar)  16. (dm+sm)   INTEL (ifort/icc)
                                         17. (dm+sm)   INTEL (ifort/icc): Xeon Phi (MIC architecture)
 18. (serial)  19. (smpar)  20. (dmpar)  21. (dm+sm)   INTEL (ifort/icc): Xeon (SNB with AVX mods)
 22. (serial)  23. (smpar)  24. (dmpar)  25. (dm+sm)   INTEL (ifort/icc): SGI MPT
 26. (serial)  27. (smpar)  28. (dmpar)  29. (dm+sm)   INTEL (ifort/icc): IBM POE
 30. (serial)               31. (dmpar)                PATHSCALE (pathf90/pathcc)
 32. (serial)  33. (smpar)  34. (dmpar)  35. (dm+sm)   GNU (gfortran/gcc)
 36. (serial)  37. (smpar)  38. (dmpar)  39. (dm+sm)   IBM (xlf90_r/cc_r)
 40. (serial)  41. (smpar)  42. (dmpar)  43. (dm+sm)   PGI (ftn/gcc): Cray XC CLE
 44. (serial)  45. (smpar)  46. (dmpar)  47. (dm+sm)   CRAY CCE (ftn $(NOOMP)/cc): Cray XE and XC
 48. (serial)  49. (smpar)  50. (dmpar)  51. (dm+sm)   INTEL (ftn/icc): Cray XC
 52. (serial)  53. (smpar)  54. (dmpar)  55. (dm+sm)   PGI (pgf90/pgcc)
 56. (serial)  57. (smpar)  58. (dmpar)  59. (dm+sm)   PGI (pgf90/gcc): -f90=pgf90
 60. (serial)  61. (smpar)  62. (dmpar)  63. (dm+sm)   PGI (pgf90/pgcc): -f90=pgf90
 64. (serial)  65. (smpar)  66. (dmpar)  67. (dm+sm)   INTEL (ifort/icc): HSW/BDW
 68. (serial)  69. (smpar)  70. (dmpar)  71. (dm+sm)   INTEL (ifort/icc): KNL MIC
 72. (serial)  73. (smpar)  74. (dmpar)  75. (dm+sm)   FUJITSU (frtpx/fccpx): FX10/FX100 SPARC64 IXfx/Xlfx

Enter selection [1-75] : 15
Compile for nesting? (1=basic, 2=preset moves, 3=vortex following) [default 1]: 1

<><> amarel % cp -p Build_wrf/Bin/wrf.exe .
<><> amarel % cp -p Build_wrf/Bin/real.exe .

Edit links.sh to point to your WRF directory
<><> amarel % links.sh                       ! removes unnecessary WRF data links created during compilation

(2) To configure WPS:
Note: Edit the namelist.wps in your case directory and it will get copied to wps at the start of the run
        cd into WPS
        ./configure
        19
        "Your versions of Fortran and Netcdf are not consistent"

        Edit configure.wps
                Change WRF_LIB from
                        WRF_LIB         =       -L$(WRF_DIR)/external/io_grib1 -lio_grib1 \
                                -L$(WRF_DIR)/external/io_grib_share -lio_grib_share \
                                -L$(WRF_DIR)/external/io_int -lwrfio_int \
                                -L$(WRF_DIR)/external/io_netcdf -lwrfio_nf \
                                -L$(NETCDF)/lib -lnetcdff -lnetcdf
                to
                        WRF_LIB         =       -L$(WRF_DIR)/external/io_grib1 -lio_grib1 \
                                -L$(WRF_DIR)/external/io_grib_share -lio_grib_share \
                                -L$(WRF_DIR)/external/io_int -lwrfio_int \
                                -L$(WRF_DIR)/external/io_netcdf -lwrfio_nf \
                                -L$(NETCDF)/lib -lnetcdff -lnetcdf \
                                -L$(HDF5_DIR)/lib -lhdf5_hl -lhdf5 -lz -lcurl

To compile WPS:
        cd into WPS
        ./compile | tee compile.log
        check that geogrid etc. are not 0K files using ls -lh


(3) To compile ROMS and the coupling system using WRF atmosphere Surface Boundary Layer (SBL),
    I made sure that the following CPP options were commented in build_roms.sh:

#setenv MY_CPP_FLAGS "${MY_CPP_FLAGS} -DBULK_FLUXES"
#setenv MY_CPP_FLAGS "${MY_CPP_FLAGS} -DCOOL_SKIN"
#setenv MY_CPP_FLAGS "${MY_CPP_FLAGS} -DWIND_MINUS_CURRENT"
#setenv MY_CPP_FLAGS "${MY_CPP_FLAGS} -DEMINUSP"
#setenv MY_CPP_FLAGS "${MY_CPP_FLAGS} -DLONGWAVE_OUT"

   Then, executed:

<><> amarel % build_roms.sh -j 8
<><> amarel % mv Buil_roms Buil_roms_atmsbl
<><> amarel % mv romsM romsM_atmsbl

     To compile ROMS and the coupling system using ROMS bulk fluxes, I uncommented the above
     CPP options in build_roms.sh:

 setenv MY_CPP_FLAGS "${MY_CPP_FLAGS} -DBULK_FLUXES"
 setenv MY_CPP_FLAGS "${MY_CPP_FLAGS} -DCOOL_SKIN"
 setenv MY_CPP_FLAGS "${MY_CPP_FLAGS} -DWIND_MINUS_CURRENT"
 setenv MY_CPP_FLAGS "${MY_CPP_FLAGS} -DEMINUSP"
 setenv MY_CPP_FLAGS "${MY_CPP_FLAGS} -DLONGWAVE_OUT"

<><> amarel % build_roms.sh -j 8
<><> amarel % mv Buil_roms Buil_roms_bulk
<><> amarel % mv romsM romsM_bulk

(3) To submit the job to the SLURM, I execute the submit BASH script to run on 16 CPUs:

<><> amarel % sbatch submit.bash

    I use 'squeue -p p_omg_1'  to check our group jobs (including JOBID)
    I use 'scancel JOBID'      to cancel a job

    It generated 3 standard output files:

    log.emsf                  Information about ESMF/NUOPC
    log.roms                  ROMS standard output files
    log.01                    coupling infrastructe set-up and WRF standard output

(4) Processing debugging output
	EDIT organize_output.sh to name your files whatever you want
	After the model is finished, if using debug level 3, run organize_output.sh
	Might need to edit that bash script for a case where there isnt a high debug level

